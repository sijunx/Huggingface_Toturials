{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3362a434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9600, '选择珠江花园的原因就是方便，有电动扶梯直', 'IE浏览器,触摸屏烫手,上面可以煎鸡蛋,', 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "import random\n",
    "\n",
    "\n",
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "#         dataset = load_dataset(path='seamew/ChnSentiCorp', split=split)\n",
    "\n",
    "#         def f(data):\n",
    "#             return len(data['text']) > 40\n",
    "\n",
    "#         self.dataset = dataset.filter(f)\n",
    "        dataset = load_from_disk('/Users/zard/Documents/nlp002/Huggingface_Toturials/data/ChnSentiCorp')\n",
    "        self.dataset = dataset[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "\n",
    "        #切分一句话为前半句和后半句\n",
    "        sentence1 = text[:20]\n",
    "        sentence2 = text[20:40]\n",
    "        label = 0\n",
    "\n",
    "        #有一半的概率把后半句替换为一句无关的话\n",
    "        if random.randint(0, 1) == 0:\n",
    "            j = random.randint(0, len(self.dataset) - 1)\n",
    "            sentence2 = self.dataset[j]['text'][20:40]\n",
    "            label = 1\n",
    "\n",
    "        return sentence1, sentence2, label\n",
    "\n",
    "\n",
    "dataset = Dataset('train')\n",
    "\n",
    "sentence1, sentence2, label = dataset[0]\n",
    "\n",
    "len(dataset), sentence1, sentence2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e70a58c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#加载字典和分词工具\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e59695a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据data: [('女人，尤其是女孩，心思细密，敏感浪漫，更', '容易好高务远，一想天开。这本书就是让女人', 0), ('商品描述与实物一致，确实是惠普的商用顶级', '用时间。集成的显卡稍微弱一些。', 1), ('整体感觉不如IdeaPad的本本。USB', '卡收货了。搞得我还要自己去话说维修点检测', 1), ('1 整体做工精细 2 白色的基本不会有手', '印的困扰，这是我选择白色的主要原因 3 ', 0), ('插上电源开机。发现电源按钮旁边有划痕，联', '。 地点十分方便：旁边是大马路解放路，往', 1), ('酒店的位置非常好，过了街就是繁华的南京路', '，离地铁站也特别近。酒店的硬件在4星级里', 0), ('环境一流，庭院深深，小桥流水，枯藤老树昏', '鸦。多次入住。只是早餐极其惨淡，无论中西', 0), ('临街.非常的吵.酒店的负责清洁的都是顺手', '牵羊或者无德之辈.共住了三次好像.前二次', 0)]\n",
      "句子sents: [('女人，尤其是女孩，心思细密，敏感浪漫，更', '容易好高务远，一想天开。这本书就是让女人'), ('商品描述与实物一致，确实是惠普的商用顶级', '用时间。集成的显卡稍微弱一些。'), ('整体感觉不如IdeaPad的本本。USB', '卡收货了。搞得我还要自己去话说维修点检测'), ('1 整体做工精细 2 白色的基本不会有手', '印的困扰，这是我选择白色的主要原因 3 '), ('插上电源开机。发现电源按钮旁边有划痕，联', '。 地点十分方便：旁边是大马路解放路，往'), ('酒店的位置非常好，过了街就是繁华的南京路', '，离地铁站也特别近。酒店的硬件在4星级里'), ('环境一流，庭院深深，小桥流水，枯藤老树昏', '鸦。多次入住。只是早餐极其惨淡，无论中西'), ('临街.非常的吵.酒店的负责清洁的都是顺手', '牵羊或者无德之辈.共住了三次好像.前二次')]\n",
      "标签labels: [0, 1, 1, 0, 1, 0, 0, 0]\n",
      "张量labels: tensor([0, 1, 1, 0, 1, 0, 0, 0])\n",
      "1200\n",
      "[CLS] 女 人 ， 尤 其 是 女 孩 ， 心 思 细 密 ， 敏 感 浪 漫 ， 更 [SEP] 容 易 好 高 务 远 ， 一 想 天 开 。 这 本 书 就 是 让 女 人 [SEP] [PAD] [PAD]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 45]),\n",
       " torch.Size([8, 45]),\n",
       " torch.Size([8, 45]),\n",
       " tensor([0, 1, 1, 0, 1, 0, 0, 0]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    \n",
    "#     print(\"数据data:\", data)\n",
    "    \n",
    "    sents = [i[:2] for i in data]\n",
    "    labels = [i[2] for i in data]\n",
    "    \n",
    "#     print(\"句子sents:\", sents)\n",
    "#     print(\"标签labels:\", labels)\n",
    "    \n",
    "    #编码\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=45,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True,\n",
    "                                   add_special_tokens=True)\n",
    "\n",
    "    #input_ids:编码之后的数字\n",
    "    #attention_mask:是补零的位置是0,其他位置是1\n",
    "    #token_type_ids:第一个句子和特殊符号的位置是0,第二个句子的位置是1\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "#     print(\"张量labels:\", labels)\n",
    "    \n",
    "    #print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=8,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "print(len(loader))\n",
    "print(token.decode(input_ids[0]))\n",
    "input_ids.shape, attention_mask.shape, token_type_ids.shape, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f620d0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d89ce99b49488c9af34f1b61147251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/393M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 45, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "#加载预训练模型\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese')\n",
    "\n",
    "#不训练,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "#模型试算\n",
    "out = pretrained(input_ids=input_ids,\n",
    "           attention_mask=attention_mask,\n",
    "           token_type_ids=token_type_ids)\n",
    "\n",
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9151282d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             token_type_ids=token_type_ids)\n",
    "\n",
    "        out = self.fc(out.last_hidden_state[:, 0])\n",
    "\n",
    "        out = out.softmax(dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model(input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bd44a7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zard/opt/anaconda3/envs/transformers001/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6829604506492615 0.5\n",
      "5 0.5733257532119751 0.875\n",
      "10 0.6138834953308105 0.75\n",
      "15 0.4034173786640167 1.0\n",
      "20 0.4893438220024109 0.875\n",
      "25 0.4256744086742401 1.0\n",
      "30 0.44798508286476135 0.875\n",
      "35 0.49994683265686035 0.875\n",
      "40 0.3902716338634491 1.0\n",
      "45 0.47605112195014954 0.875\n",
      "50 0.38127291202545166 1.0\n",
      "55 0.46472567319869995 0.875\n",
      "60 0.6686863899230957 0.5\n",
      "65 0.4596948027610779 0.75\n",
      "70 0.4620363712310791 0.875\n",
      "75 0.4030296504497528 1.0\n",
      "80 0.38339099287986755 1.0\n",
      "85 0.4915534257888794 0.75\n",
      "90 0.41055816411972046 0.875\n",
      "95 0.578237771987915 0.75\n",
      "100 0.3514157831668854 1.0\n",
      "105 0.4582573473453522 0.875\n",
      "110 0.41087818145751953 1.0\n",
      "115 0.5620350241661072 0.75\n",
      "120 0.5009777545928955 0.875\n",
      "125 0.4349495768547058 0.875\n",
      "130 0.4879797697067261 0.75\n",
      "135 0.47779300808906555 0.875\n",
      "140 0.4227628707885742 0.875\n",
      "145 0.35516491532325745 1.0\n",
      "150 0.5687816739082336 0.625\n",
      "155 0.43313172459602356 0.875\n",
      "160 0.374252051115036 0.875\n",
      "165 0.3627183139324188 1.0\n",
      "170 0.4084283113479614 0.875\n",
      "175 0.46436724066734314 0.75\n",
      "180 0.3862093687057495 1.0\n",
      "185 0.35506242513656616 1.0\n",
      "190 0.508195161819458 0.75\n",
      "195 0.420492947101593 1.0\n",
      "200 0.32509952783584595 1.0\n",
      "205 0.47463691234588623 0.75\n",
      "210 0.6169066429138184 0.5\n",
      "215 0.32532691955566406 1.0\n",
      "220 0.6087907552719116 0.625\n",
      "225 0.4706989526748657 0.875\n",
      "230 0.5499688386917114 0.75\n",
      "235 0.5491138696670532 0.75\n",
      "240 0.33294516801834106 1.0\n",
      "245 0.570624589920044 0.625\n",
      "250 0.741696834564209 0.5\n",
      "255 0.5596460700035095 0.75\n",
      "260 0.42041122913360596 0.875\n",
      "265 0.5663917660713196 0.75\n",
      "270 0.3268616497516632 1.0\n",
      "275 0.43616700172424316 0.875\n",
      "280 0.37332701683044434 1.0\n",
      "285 0.4747787415981293 0.875\n",
      "290 0.49630603194236755 0.75\n",
      "295 0.4552305340766907 0.875\n",
      "300 0.5834873914718628 0.625\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "#训练\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "    out = model(input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        out = out.argmax(dim=1)\n",
    "        accuracy = (out == labels).sum().item() / len(labels)\n",
    "        print(i, loss.item(), accuracy)\n",
    "\n",
    "    if i == 300:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "275dd1b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.925\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('test'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader_test):\n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "        print(i)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        pred = out.argmax(dim=1)\n",
    "\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    print(correct / total)\n",
    "\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
