{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a9c0123",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 1., 1., 1.])\n",
      "x.shape: torch.Size([4])\n",
      "x: tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "--------------------\n",
      "y: tensor([2., 2., 2., 2.], grad_fn=<AddBackward0>)\n",
      "数值x: tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "x.grade: tensor([ 6.,  9., 12.,  3.])\n",
      "grad_variables.grade: None\n",
      "z: tensor([3., 3., 3., 3.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# https://blog.csdn.net/Mr_zhuo_/article/details/108132061\n",
    "\n",
    "x = torch.ones(4)\n",
    "print(\"x:\", x)\n",
    "\n",
    "x = Variable(x, requires_grad=True)\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"x:\", x)\n",
    "print(\"--------------------\")\n",
    "y = x**2 + x\n",
    "print(\"y:\", y)\n",
    "#梯度参数grad_variables形状必须与Variable一致\n",
    "grad_variables = torch.FloatTensor([2, 3, 4, 1]) \n",
    "\n",
    "y.backward(grad_variables)\n",
    "#反向传播原理示例：https://blog.csdn.net/deephub/article/details/115438881\n",
    "print(\"数值x:\", x)\n",
    "print(\"x.grade:\", x.grad)\n",
    "print(\"grad_variables.grade:\", grad_variables.grad)\n",
    "z = 2*x + 1\n",
    "print(\"z:\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "844f2daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.], grad_fn=<PowBackward0>)\n",
      "tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "#自动求导机制\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# 1、简单的求导(求导对象是标量)\n",
    "x = Variable(torch.Tensor([2]),requires_grad=True)\n",
    "y = x**2\n",
    "print(y)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "deb554aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1311, -0.1033, -0.0323, -0.0786, -0.1015, -0.1312,  0.0653, -0.0382,\n",
      "          0.0502,  0.0403,  0.0675,  0.1925, -0.0342,  0.0982,  0.1353, -0.0196,\n",
      "          0.0238, -0.1999, -0.0284,  0.0772],\n",
      "        [ 0.1311, -0.1033, -0.0323, -0.0786, -0.1015, -0.1312,  0.0653, -0.0382,\n",
      "          0.0502,  0.0403,  0.0675,  0.1925, -0.0342,  0.0982,  0.1353, -0.0196,\n",
      "          0.0238, -0.1999, -0.0284,  0.0772],\n",
      "        [ 0.1311, -0.1033, -0.0323, -0.0786, -0.1015, -0.1312,  0.0653, -0.0382,\n",
      "          0.0502,  0.0403,  0.0675,  0.1925, -0.0342,  0.0982,  0.1353, -0.0196,\n",
      "          0.0238, -0.1999, -0.0284,  0.0772],\n",
      "        [ 0.1311, -0.1033, -0.0323, -0.0786, -0.1015, -0.1312,  0.0653, -0.0382,\n",
      "          0.0502,  0.0403,  0.0675,  0.1925, -0.0342,  0.0982,  0.1353, -0.0196,\n",
      "          0.0238, -0.1999, -0.0284,  0.0772],\n",
      "        [ 0.1311, -0.1033, -0.0323, -0.0786, -0.1015, -0.1312,  0.0653, -0.0382,\n",
      "          0.0502,  0.0403,  0.0675,  0.1925, -0.0342,  0.0982,  0.1353, -0.0196,\n",
      "          0.0238, -0.1999, -0.0284,  0.0772],\n",
      "        [ 0.1311, -0.1033, -0.0323, -0.0786, -0.1015, -0.1312,  0.0653, -0.0382,\n",
      "          0.0502,  0.0403,  0.0675,  0.1925, -0.0342,  0.0982,  0.1353, -0.0196,\n",
      "          0.0238, -0.1999, -0.0284,  0.0772],\n",
      "        [ 0.1311, -0.1033, -0.0323, -0.0786, -0.1015, -0.1312,  0.0653, -0.0382,\n",
      "          0.0502,  0.0403,  0.0675,  0.1925, -0.0342,  0.0982,  0.1353, -0.0196,\n",
      "          0.0238, -0.1999, -0.0284,  0.0772],\n",
      "        [ 0.1311, -0.1033, -0.0323, -0.0786, -0.1015, -0.1312,  0.0653, -0.0382,\n",
      "          0.0502,  0.0403,  0.0675,  0.1925, -0.0342,  0.0982,  0.1353, -0.0196,\n",
      "          0.0238, -0.1999, -0.0284,  0.0772],\n",
      "        [ 0.1311, -0.1033, -0.0323, -0.0786, -0.1015, -0.1312,  0.0653, -0.0382,\n",
      "          0.0502,  0.0403,  0.0675,  0.1925, -0.0342,  0.0982,  0.1353, -0.0196,\n",
      "          0.0238, -0.1999, -0.0284,  0.0772],\n",
      "        [ 0.1311, -0.1033, -0.0323, -0.0786, -0.1015, -0.1312,  0.0653, -0.0382,\n",
      "          0.0502,  0.0403,  0.0675,  0.1925, -0.0342,  0.0982,  0.1353, -0.0196,\n",
      "          0.0238, -0.1999, -0.0284,  0.0772]])\n",
      "tensor([[0.1000],\n",
      "        [0.1000],\n",
      "        [0.1000],\n",
      "        [0.1000],\n",
      "        [0.1000],\n",
      "        [0.1000],\n",
      "        [0.1000],\n",
      "        [0.1000],\n",
      "        [0.1000],\n",
      "        [0.1000]])\n",
      "tensor([[-0.0263],\n",
      "        [ 0.5159],\n",
      "        [-0.0069],\n",
      "        [-0.2550],\n",
      "        [ 0.2674],\n",
      "        [ 0.5478],\n",
      "        [ 0.2850],\n",
      "        [ 0.4534],\n",
      "        [-0.6595],\n",
      "        [-0.0973],\n",
      "        [-0.2052],\n",
      "        [ 0.4103],\n",
      "        [ 0.4759],\n",
      "        [ 0.2493],\n",
      "        [ 0.2803],\n",
      "        [-0.5186],\n",
      "        [-0.3548],\n",
      "        [ 0.4743],\n",
      "        [-0.4336],\n",
      "        [ 0.2177]])\n"
     ]
    }
   ],
   "source": [
    "#对矩阵求导\n",
    "x1 = Variable(torch.randn(10, 20), requires_grad=True)\n",
    "y1 = Variable(torch.randn(10, 1), requires_grad=True)\n",
    "W = Variable(torch.randn(20, 1), requires_grad=True)\n",
    "\n",
    "#matmul表示做矩阵乘法\n",
    "J = torch.mean(y1 - torch.matmul(x1, W))\n",
    "J.backward()\n",
    "\n",
    "\n",
    "print(x1.grad)\n",
    "print(y1.grad)\n",
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c18527a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.])\n",
      "tensor([16.])\n"
     ]
    }
   ],
   "source": [
    "#复杂情况的自动求导 多维数组自动求导机制\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.FloatTensor([3]),requires_grad=True)\n",
    "y = x ** 2 + x * 2 + 3\n",
    "y.backward(retain_graph=True) #保留计算图\n",
    "print(x.grad)\n",
    "y.backward()#不保留计算图\n",
    "print(x.grad) #得到的是第一次求导的值加上第二次求导的值 8 + 8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
